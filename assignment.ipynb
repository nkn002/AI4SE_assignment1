{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"java\", cache_dir='saved_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 164923/164923 [00:58<00:00, 2838.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 164923/164923 [02:31<00:00, 1090.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "class NGramLM:\n",
    "    def __init__(self, corpus, model='codellama/CodeLlama-7b-hf'):\n",
    "        self.corpus = corpus\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model, cache_dir='./tokenizers')\n",
    "        self.unigram_model, self.bigram_model, self.trigram_model, self.fourgram_model = self.build_ngrams()\n",
    "        \n",
    "        \n",
    "    def build_ngrams(self):\n",
    "        # Initialize models for n-grams\n",
    "        unigram_model =  {}\n",
    "        bigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        trigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        fourgram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "        # Build n-gram models from the corpus\n",
    "        for words in tqdm(self.corpus):\n",
    "            if isinstance(words, str):\n",
    "                words = [token.replace('▁', '') for token in self.tokenizer.tokenize(words)]\n",
    "            \n",
    "            for word in words:\n",
    "                if word not in unigram_model:\n",
    "                    unigram_model[word] =0\n",
    "                else:\n",
    "                    unigram_model[word] += 1\n",
    "                \n",
    "            # Build bigram model\n",
    "            bi_grams = list(bigrams(words))\n",
    "            for w1, w2 in bi_grams:\n",
    "                bigram_model[w1][w2] += 1\n",
    "\n",
    "            # Build trigram model\n",
    "            tri_grams = list(trigrams(words))\n",
    "            for w1, w2, w3 in tri_grams:\n",
    "                trigram_model[(w1, w2)][w3] += 1\n",
    "\n",
    "            # Build four-gram model\n",
    "            four_grams = list(ngrams(words, 4))\n",
    "            for w1, w2, w3, w4 in four_grams:\n",
    "                fourgram_model[(w1, w2, w3)][w4] += 1\n",
    "\n",
    "        # Normalize all n-gram models\n",
    "        self.normalize_ngrams(bigram_model, trigram_model, fourgram_model)\n",
    "    \n",
    "        total_words = sum(list(unigram_model.values()))\n",
    "        threshold = (max(unigram_model.values()) + min(unigram_model.values())) // 2\n",
    "        unigram_model = {i:(j/total_words + 1e-10) for i,j in unigram_model.items() if j > threshold}\n",
    "        return unigram_model, bigram_model, trigram_model, fourgram_model\n",
    "\n",
    "    # Normalization function for all n-gram models\n",
    "    def normalize_ngrams(self, *ngram_models):\n",
    "        for model in ngram_models:\n",
    "            for context in model:\n",
    "                total_count = float(sum(model[context].values()))\n",
    "                for word in model[context]:\n",
    "                    model[context][word] /= total_count\n",
    "                    model[context][word] += 1e-10\n",
    "\n",
    "    # Function to predict the next word using five-gram, four-gram, trigram, or bigram\n",
    "        # Function to predict the next word\n",
    "    def predict_next_word(self, w1, w2, w3):\n",
    "        next_word = self.fourgram_model[(w1, w2, w3)]\n",
    "        if next_word:\n",
    "            predicted_word = max(next_word, key=next_word.get)\n",
    "            return predicted_word\n",
    "        else:\n",
    "            next_word = self.trigram_model[(w2, w3)]\n",
    "            if next_word:\n",
    "                predicted_word = max(next_word, key=next_word.get)\n",
    "                return predicted_word\n",
    "            else:\n",
    "                next_word = self.bigram_model[w3]\n",
    "                if next_word:\n",
    "                    predicted_word = max(next_word, key=next_word.get)\n",
    "                    return predicted_word\n",
    "                else:\n",
    "                    return max(self.unigram_model, key=self.unigram_model.get)\n",
    "        return \"UNK\"\n",
    "\n",
    "\n",
    "    def compute_perplexity(self, test_data, tokenized=True):\n",
    "        if isinstance(test_data, str):\n",
    "            if tokenized:\n",
    "                test_data = test_data.split()\n",
    "            else:\n",
    "                test_data = [token.replace('▁', '') for token in self.tokenizer.tokenize(test_data)]\n",
    "        test_fourgrams = list(ngrams(test_data, 4))\n",
    "        N = len(test_fourgrams)\n",
    "        log_prob_sum = 0\n",
    "\n",
    "        for w1, w2, w3, w4 in test_fourgrams:\n",
    "            prob = self.fourgram_model[(w1, w2, w3)].get(w4, 'N/A')\n",
    "            if prob == 'N/A':                \n",
    "                prob = self.trigram_model[(w2, w3)].get(w4, 'N/A')\n",
    "            if prob == 'N/A':\n",
    "                prob = self.bigram_model[w3].get(w4, 'N/A')\n",
    "            if prob == 'N/A':\n",
    "                prob = self.unigram_model.get(w4, 1e-10)\n",
    "            log_prob_sum += math.log(prob, 2)\n",
    "\n",
    "        # Calculate the perplexity\n",
    "        perplexity = 2 ** (-log_prob_sum / N)\n",
    "        return perplexity\n",
    "\n",
    "    \n",
    "    \n",
    "    def predict_sentence(self, test_data, tokenized=True):\n",
    "        if isinstance(test_data, str):\n",
    "            if tokenized:\n",
    "                test_data = test_data.split()\n",
    "            else:\n",
    "                test_data = [token.replace('▁', '') for token in self.tokenizer.tokenize(test_data)]\n",
    "        test_4grams = list(ngrams(test_data, 4))\n",
    "        N = len(test_4grams)\n",
    "        log_prob_sum = 0\n",
    "        \n",
    "        correct = 0\n",
    "        for w1, w2, w3, w4 in test_4grams:\n",
    "            # Get the probability of the trigram (w1, w2, w3)\n",
    "            next_word = self.predict_next_word(w1,w2,w3)\n",
    "            if next_word == w4:\n",
    "                correct += 1\n",
    "        \n",
    "        # Calculate the perplexity\n",
    "        accuracy = correct/len(test_4grams)\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def eval_on_corpus(self, test_corpus, tokenized=True):\n",
    "        if isinstance(test_corpus, str):\n",
    "            with open(test_corpus, 'r') as f:\n",
    "                test_corpus = f.readlines()\n",
    "        \n",
    "        all_accs = []\n",
    "        all_ppls = []\n",
    "        for test_data in tqdm(test_corpus):\n",
    "            all_ppls.append(self.compute_perplexity(test_data, tokenized=tokenized))\n",
    "            all_accs.append(self.predict_sentence(test_data, tokenized=tokenized))\n",
    "            \n",
    "        avg_acc = sum(all_accs)/len(all_accs)\n",
    "        avg_ppl = sum(all_ppls)/len(all_ppls)\n",
    "        \n",
    "        print(\"Average accuracy:\", avg_acc)\n",
    "        print(\"Average perplexity:\", avg_ppl)\n",
    "        return avg_acc,avg_ppl\n",
    "\n",
    "\n",
    "ngramLM_pre = NGramLM(ds['train']['code_tokens'])\n",
    "ngramLM_codellama = NGramLM(ds['train']['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 302.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.2998862631472445\n",
      "Average perplexity: 296.52376536921287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2998862631472445, 296.52376536921287)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramLM_codellama.eval_on_corpus('all_codes.txt', tokenized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 191.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.17650285944175992\n",
      "Average perplexity: 2274776.091388698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.17650285944175992, 2274776.091388698)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramLM_codellama.eval_on_corpus('all_codes.txt', tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:14<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.24366236780749972\n",
      "Average perplexity: 661483.3497655754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.24366236780749972, 661483.3497655754)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramLM_pre.eval_on_corpus('all_codes.txt', tokenized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.45521732685478733\n",
      "Average perplexity: 928.174491875554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45521732685478733, 928.174491875554)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramLM_pre.eval_on_corpus(ds['test'][:100]['code_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if { if ( ! ( ( ( AbstractAttribute ) attribute )\n"
     ]
    }
   ],
   "source": [
    "seed = ['', 'if', '{']\n",
    "for i in range(1):\n",
    "    out = ngramLM_pre.predict_next_word(seed[-3], seed[-2], seed[-1])\n",
    "    seed += [out]\n",
    "print(' '.join(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramLM.predict_next_word('','(', '!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramLM.trigram_model[('static', 'boolean')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramLM.fourgram_model[('',\n",
    "              'boolean',\n",
    "              'check')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distill",
   "language": "python",
   "name": "distill"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
